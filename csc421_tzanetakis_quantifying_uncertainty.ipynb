{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CHAPTER 12 - Quantifying Uncertainty \n",
    "\n",
    "### George Tzanetakis, University of Victoria \n",
    "\n",
    "\n",
    "* Laziness: too much work to completely cover every possible expection \n",
    "* Theoretical ignorance \n",
    "* Practical ignorance: missing evidence, noisy observations, unexpected changes \n",
    "\n",
    "Probability is the primary formalism for dealing with uncertainty \n",
    "\n",
    "* Degree of belief  (sentence is true or false) \n",
    "    * There is a pit at square (2,2)\n",
    "    \n",
    "\n",
    "* Ace of spades example: \n",
    "    * No knowledge (1/52) \n",
    "    * Color  (1/26) \n",
    "    * Suit (1/13 ) \n",
    "    * Show card (0 or 1) \n",
    "\n",
    "\n",
    "Note: Alternative: Fuzzy Logic – degree of truth “This towel is wet” \n",
    "            \n",
    "In this notebook we will explore discrete random variables and sampling. After defining a helper class and associated functions we will be able to create both symbolic and numeric random variables and generate samples from them. \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WORKPLAN \n",
    "\n",
    "The section number is based on the 4th edition of the AIMA textbook and is the suggested\n",
    "reading for this week. Each list entry provides just the additional sections. For example the Expected reading include the sections listed under Basic as well as the sections listed under Expected. Some additional readings are suggested for Advanced. \n",
    "\n",
    "1. Basic: Sections **12.1**, **12.2**, **12.3**, **12.4**, **12.5**, **12.6**, and **Summary**\n",
    "2. Expected: Same as Basic \n",
    "3. Advanced: All the chapter including bibligraphical and historical notes \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Sidenote about language confusion: Monty Hall problem \n",
    "    \n",
    "    \n",
    "Three closed doors one of which hides the car of your dreams. Behind each of the other two goats. You will choose a door and win whatever is behind it. You decide on a door and announce your choice, whereas the hosts opens one of the other two doors and reveals a goat. He then asks if you would like to switch you choice or not. What should you do ? \n",
    "\n",
    "https://en.wikipedia.org/wiki/Monty_Hall_problem\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probability \n",
    "\n",
    "* Primitive instictive probability \n",
    "    * Dark clouds today mean rain is likely since it has rained in the past when the clouds had that look  \n",
    "* Formal theory 17-century Correspondance between B. Pascal and P. Fermat about gambling \n",
    "\n",
    "Pervasive in all sciences. Knowledge of probability is critical for any CS practitioner. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample spaces, events, worlds\n",
    "\n",
    "\n",
    "* A = {(1,6), (2,5), (3,4), (4,3), (5,2), (6,1)} \n",
    "* Rolling a seven with a pair of dice \n",
    "* Experiment gives rise to sample space \n",
    "* Discrete sample spaces (infinite but countable) \n",
    "\n",
    "* Probability distribution = associating each of the discrete outcomes with a number between 0 and 1. The sum of all the outcomes must add up to 1. \n",
    "\n",
    "\n",
    "Car-goat \n",
    "\n",
    "\n",
    "* Assume you switch S = {(1,2,3,L), (1, 3, 2, L), (2, 3, 1, W), (3, 2, 1, W)} \n",
    "* Door you choose, door hosts opens, door you switch to, win/loss \n",
    "\n",
    "Sidenote 2: Another puzzle \n",
    "\n",
    "* The king comes from a family of two children. What is the probability that the other child is his sister ? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making decisions under uncertainty \n",
    "\n",
    "Suppose I believe the following: \n",
    "* P(30min  gets me there on time) = 0.04 \n",
    "* P(1 hr gets me there on time) = 0.70 \n",
    "* P( 24 hrs gets me there on time) = 0.98 \n",
    "\n",
    "* Which action to choose ? Depends on my preferencesUtility theory = represent preferences \n",
    "* Decision theory = utility theory + probability theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probability Theory \n",
    "\n",
    "\n",
    "* Set $\\Omega$  – the sample space \n",
    "* $\\omega \\in \\Omega$ is a sample point/atomic event, outcome, possible world \n",
    "* A probability space/model is a sample space with an assignment $P(\\Omega)$ for every point such that \n",
    "    * $0.0 \\leq P(\\omega) \\leq 1.0$\n",
    "    * $\\sum_{\\omega} P(\\omega) = 1.0$ \n",
    "    \n",
    "An event is a subset of $\\Omega$. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random variables \n",
    "\n",
    "* A random variable is a function from sample points to some range e.g the reals or booleans e.g. \n",
    "* Odd(1) = true \n",
    "* P induces a probability distribution for any r.v X \n",
    "    * $P(Odd = true) = P(1) + P(3) + P(5) = \\frac{1}{2}$ \n",
    "    \n",
    "Random Variables represents a “part” of the world whose “status” is initially unknown. Each random variable has a domain that it can take on. For example, the RV Weather can have the values: sun, rain, cloud, snow. Domains can be boolen, discrete, continuous.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Probabilities are assigned over values in the domain. \n",
    "\n",
    "The notation P(Weather) denotes a vector of values for the probabilities of each individual state of the weather: \n",
    "\n",
    "* $ P(Weather = sunny) =0.65$ \n",
    "* $ P(Weather = rain) =0.25 $\n",
    "* $ P(Weather = cloudy)=0.07 $\n",
    "* $ P(Weather = snow) = 0.03 $ \n",
    "* $P(Weather) = (0.65,0.25,0.07,0.03)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Proposition \n",
    "\n",
    "* Think of a proposition as the event (set of sample points) where the proposition is true \n",
    "* Given boolen random variables A, BEvent a = set of sample points where A(w ) = 1 \n",
    "* Often in AI sample points are defined by the values of a set of random variables i.e., the sample space is the Cartesian product of the ranges of the variables \n",
    "* With boolean variables sample point = propositional logic model A = true, B = false"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frequentist interpretation \n",
    "\n",
    "An event’s probability is the limit of its relative frequency in a large number of trials. This connects to statistics and empirical experiments. The initial classical definition of probability was based on physical idealized symmetry (dice, coins, cards). The axiomatic formulation of probability by Kolmogorov (1903-1987) in 1933 focuses on operations on probability values rather than the initial assignment of values.\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"images/frequentists_bayesians.png\" width=\"60%\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: Probability theory help us calculate unknown probabilities of events based on known probabilities of other events. How the numbers are assigned to particular events is problem and domain dependent. They can be assigned based on degrees of belief or they can be estimated by statistical frequency of occurence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "\n",
    "## Random variables and sampling\n",
    "\n",
    "The probabilities associated with every possible value of a random variable consitute a probability distribution. The process of selecting a value randomly according to the probability distribution is called sampling. It can be viewed as a process of generating a sequence of random samples and it can help us better understand how a particular probabilistic model works.\n",
    "\n",
    "\n",
    "Define a helper random variable class based on the scipy discrete random variable functionality providing both numeric and symbolic RVs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline \n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class Random_Variable: \n",
    "    \n",
    "    def __init__(self, name, values, probability_distribution): \n",
    "        self.name = name \n",
    "        self.values = values \n",
    "        self.probability_distribution = probability_distribution \n",
    "        if all(type(item) is np.int64 for item in values): \n",
    "            self.type = 'numeric'\n",
    "            self.rv = stats.rv_discrete(name = name, values = (values, probability_distribution))\n",
    "        elif all(type(item) is str for item in values): \n",
    "            self.type = 'symbolic'\n",
    "            self.rv = stats.rv_discrete(name = name, values = (np.arange(len(values)), probability_distribution))\n",
    "            self.symbolic_values = values \n",
    "        else: \n",
    "            self.type = 'undefined'\n",
    "            \n",
    "    def sample(self,size): \n",
    "        if (self.type =='numeric'): \n",
    "            return self.rv.rvs(size=size)\n",
    "        elif (self.type == 'symbolic'): \n",
    "            numeric_samples = self.rv.rvs(size=size)\n",
    "            mapped_samples = [self.values[x] for x in numeric_samples]\n",
    "            return mapped_samples \n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first create some random samples of symbolic random variables corresponding to a coin and a dice "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Die\n",
      "['H', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'T', 'H', 'H', 'H', 'T', 'T', 'H', 'H', 'H', 'T', 'T', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'T', 'H', 'H', 'H', 'H']\n",
      "50\n"
     ]
    }
   ],
   "source": [
    "values = ['H', 'T']\n",
    "probabilities = [0.9, 0.1]\n",
    "coin = Random_Variable('Die', values, probabilities)\n",
    "print(coin.name)\n",
    "samples = coin.sample(50)\n",
    "print(samples)\n",
    "\n",
    "\n",
    "def length(samples): \n",
    "    length=0\n",
    "    for x in samples: \n",
    "        length = length+1 \n",
    "    return length\n",
    "    \n",
    "print(length(samples))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['6', '5', '4', '5', '5', '2', '6', '2', '4', '4', '5', '5', '2', '3', '2', '1', '5', '3', '4', '1', '4', '5', '5', '2', '3', '6', '4', '2', '4', '4']\n"
     ]
    }
   ],
   "source": [
    "values = ['1', '2', '3', '4', '5', '6']\n",
    "probabilities = [1/6.] * 6\n",
    "probabilities = [1/6., 1/6., 1/6., 1/6., 1/6., 1/6.]\n",
    "dice = Random_Variable('dice', values, probabilities)\n",
    "samples = dice.sample(30)\n",
    "print(samples);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's look at a numeric random variable corresponding to a dice so that we can more easily make plots and histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 3 4 5 6]\n"
     ]
    }
   ],
   "source": [
    "values = np.arange(1,7)\n",
    "probabilities = [1/6.] * 6\n",
    "print(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<StemContainer object of 3 artists>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "dice = Random_Variable('dice', values, probabilities)\n",
    "samples = dice.sample(1000)\n",
    "plt.stem(samples, markerfmt= ' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now look at a histogram of these generated samples. Notice that even with 500 samples the bars are not equal length so the calculated frequencies are only approximating the probabilities used to generate them "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.hist(samples,bins=[1,2,3,4,5,6,7],normed=1, rwidth=0.5,align='left');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the cumulative histogram of the samples "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(samples,bins=[1,2,3,4,5,6,7],normed=1, rwidth=0.5,align='left', cumulative=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now estimate the frequency of the event *roll even number* in different ways. \n",
    "First let's count the number of even numbers in the generated samples. Then let's \n",
    "take the sum of the counts of the individual estimated probabilities. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can also write the predicates directly using lambda notation \n",
    "samples = dice.sample(500)\n",
    "est_even = len([x for x in samples if x%2==0]) / len(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.508\n",
      "Estimates of 2,4,6 =  (0.186, 0.154, 0.168)\n",
      "Direct estimate =  0.508\n",
      "Sum of estimates =  0.508\n",
      "Theoretical value =  0.5\n",
      "0.186\n"
     ]
    }
   ],
   "source": [
    "est_2 = len([x for x in samples if x==2]) / len(samples)\n",
    "est_4 = len([x for x in samples if x==4]) / len(samples)\n",
    "est_6 = len([x for x in samples if x==6]) / len(samples)\n",
    "print(est_even)\n",
    "# Let's print some estimates \n",
    "print('Estimates of 2,4,6 = ', (est_2, est_4, est_6))\n",
    "print('Direct estimate = ', est_even) \n",
    "print('Sum of estimates = ', est_2 + est_4 + est_6)\n",
    "print('Theoretical value = ', 0.5)\n",
    "\n",
    "est_2 = 0 \n",
    "for x in samples: \n",
    "    if x == 2: est_2 = est_2 + 1 \n",
    "print(est_2/len(samples))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Notice that we can always estimate the probability of an event by simply counting how many times it occurs in the samples of an experiment. However if we have multiple events we are interested in then it can be easier to calculate the probabilities of the values of invdividual random variables and then use the rules of probability to estimate the probabilities of more complex events. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sidenote: A probabilistic view of machine learning \n",
    "\n",
    "\n",
    "The basic recipe: \n",
    "\n",
    "* Describe how the data is generated and the assumptions you make using a probabilistic model \n",
    "* Estimate the parameters of the probabilistic model using available data (the learning part) \n",
    "* Use the estimated probabilistic model to perform various tasks. \n",
    "* Evaluate how well the model performs \n",
    "\n",
    "\n",
    "Some important observations: \n",
    "\n",
    "* Understanding notation in addition to the underlying concepts is important \n",
    "* Separating model from inference \n",
    "* Understanding the connection between statistics and probability \n",
    "* Thinking the generative way \n",
    "* Probabilistic modeling is all about how to calculate probabilities of events that are “hard” to estimate from probabilities of events that are “easier” to estimate \n",
    "* Focus on the basic concepts and don’t get bogged down in the implementation details and the multiple variants \n",
    "* Misleading use of language is frequently why probability problems can be difficult (for example Monty Hall). In most applications that’s not a problem.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conditional probabilities \n",
    "\n",
    "We explore conditional probabilities using an example from music. Imagine that you have a collection of songs consisting of two genres: country and jazz. Some songs have lyrics and some have not i.e they are instrumental. It makes sense that the probability of a song being instrumental depends on whether the song is jazz or country. This can be modeled through conditional probabilities.\n",
    "\n",
    "We can simulate the generation process of conditional probabilities by appropriately sampling from three random variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we introduce another random variable hasLyrics with two values: no and yes. We expect that more country songs will have lyrics than jazz songs. That means that the probability distribution of hasLyrics depends on whether the genre is country or jazz. This is known as a conditional probability distribution and is notated as follows: $P(hasLyrics = no/genre = jazz) = 0.9$ This implies that $P(hasLyrics = yes|genre = jazz) = 0.1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If genre = country then we have: $P(hasLyrics = no|genre = country) = 0.2$ We can use the short-hand notation $P(hasLyrics|genre)$ to denote the conditional probability distribution that in this case can be specified by providing four probabilities (or two using normalization). We will call these numbers and in general any numbers used to “specify” a particular probabilistic model parameters and use $\\theta$ to denote a vector containing them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can display all the relevant probabilities using a conditional probability table. Notice that the sum of row entries must be equal to 1 but NOT the sum of column entries. \n",
    "\n",
    "| Genre/Lyrics | no  | yes |\n",
    "|--------------|-----|-----|\n",
    "| country      | 0.2 | 0.8 |\n",
    "| jazz         | 0.9 | 0.1 |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note about notation** \n",
    "\n",
    "Frequently when notating a conditional probablity distribution the short hand $P(hasLyrics|genre)$ is used. Conceptually this expands to all possible combinations of values of the two random variables involved. Also some times when the values of random variables in a problem are unique the name of the random variable is omitted i.e P(country) instead of P(genre = country). It is important to keep in mind these conventions as our examples get more complicated.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('no', 'jazz')\n",
      "('yes', 'country')\n",
      "('no', 'country')\n",
      "('yes', 'jazz')\n",
      "('yes', 'country')\n",
      "('yes', 'country')\n",
      "('yes', 'country')\n",
      "('yes', 'country')\n",
      "('yes', 'jazz')\n",
      "('yes', 'country')\n",
      "('yes', 'country')\n",
      "('yes', 'country')\n",
      "('yes', 'country')\n",
      "('yes', 'country')\n",
      "('no', 'country')\n",
      "('yes', 'country')\n",
      "('no', 'jazz')\n",
      "('yes', 'country')\n",
      "('yes', 'country')\n",
      "('no', 'jazz')\n",
      "('yes', 'country')\n",
      "('yes', 'country')\n",
      "('yes', 'country')\n",
      "('no', 'jazz')\n",
      "('no', 'country')\n",
      "('no', 'country')\n",
      "('yes', 'country')\n",
      "('yes', 'country')\n",
      "('no', 'jazz')\n",
      "('yes', 'jazz')\n",
      "('yes', 'country')\n",
      "('no', 'jazz')\n",
      "('yes', 'country')\n",
      "('no', 'country')\n",
      "('yes', 'country')\n",
      "('no', 'jazz')\n",
      "('no', 'jazz')\n",
      "('yes', 'country')\n",
      "('no', 'country')\n",
      "('yes', 'country')\n",
      "('no', 'jazz')\n",
      "('no', 'jazz')\n",
      "('yes', 'country')\n",
      "('no', 'country')\n",
      "('yes', 'country')\n",
      "('yes', 'country')\n",
      "('no', 'country')\n",
      "('yes', 'country')\n",
      "('yes', 'country')\n",
      "('no', 'country')\n",
      "('no', 'jazz')\n",
      "('yes', 'country')\n",
      "('yes', 'country')\n",
      "('yes', 'country')\n",
      "('no', 'jazz')\n",
      "('yes', 'country')\n",
      "('yes', 'country')\n",
      "('yes', 'country')\n",
      "('yes', 'country')\n",
      "('yes', 'country')\n",
      "('no', 'country')\n",
      "('no', 'country')\n",
      "('no', 'country')\n",
      "('no', 'jazz')\n",
      "('yes', 'country')\n",
      "('yes', 'country')\n",
      "('yes', 'country')\n",
      "('yes', 'country')\n",
      "('no', 'jazz')\n",
      "('yes', 'country')\n",
      "('no', 'jazz')\n",
      "('no', 'jazz')\n",
      "('yes', 'country')\n",
      "('yes', 'country')\n",
      "('no', 'jazz')\n",
      "('yes', 'country')\n",
      "('yes', 'country')\n",
      "('no', 'jazz')\n",
      "('yes', 'jazz')\n",
      "('yes', 'country')\n",
      "('no', 'jazz')\n",
      "('yes', 'country')\n",
      "('yes', 'country')\n",
      "('yes', 'country')\n",
      "('no', 'jazz')\n",
      "('yes', 'country')\n",
      "('yes', 'country')\n",
      "('no', 'jazz')\n",
      "('no', 'jazz')\n",
      "('yes', 'country')\n",
      "('no', 'jazz')\n",
      "('no', 'jazz')\n",
      "('yes', 'country')\n",
      "('yes', 'country')\n",
      "('yes', 'country')\n",
      "('yes', 'country')\n",
      "('no', 'jazz')\n",
      "('yes', 'country')\n",
      "('yes', 'country')\n",
      "('yes', 'country')\n"
     ]
    }
   ],
   "source": [
    "# samples to generate \n",
    "num_samples = 1000\n",
    "\n",
    "## Prior probabilities of a song being jazz or country \n",
    "values = ['country', 'jazz']\n",
    "probs = [0.7, 0.3]\n",
    "genre = Random_Variable('genre',values, probs)\n",
    "\n",
    "\n",
    "# conditional probabilities of a song having lyrics or not given the genre \n",
    "values = ['no', 'yes']\n",
    "probs = [0.9, 0.1] \n",
    "lyrics_if_jazz = Random_Variable('lyrics_if_jazz', values, probs)\n",
    "\n",
    "values = ['no', 'yes']\n",
    "probs = [0.2, 0.8]\n",
    "lyrics_if_country = Random_Variable('lyrics_if_country', values, probs)\n",
    "\n",
    "# conditional generating proces first sample prior and then based on outcome \n",
    "# choose which conditional probability distribution to use \n",
    "\n",
    "random_lyrics_samples = [] \n",
    "for n in range(num_samples): \n",
    "    # the 1 below is to get one sample and the 0 to get the first item of the list of samples \n",
    "    random_genre_sample = genre.sample(1)[0]\n",
    "    # depending on the outcome of the genre sampling sample the appropriate \n",
    "    # conditional probability \n",
    "    if (random_genre_sample == 'jazz'): \n",
    "        random_lyrics_sample = (lyrics_if_jazz.sample(1)[0], 'jazz')\n",
    "        # random_lyrics_sample = (lyrics_if_jazz.sample(1)[0])\n",
    "\n",
    "    else: \n",
    "        random_lyrics_sample = (lyrics_if_country.sample(1)[0], 'country')\n",
    "        # random_lyrics_sample = (lyrics_if_country.sample(1)[0])\n",
    "\n",
    "    random_lyrics_samples.append(random_lyrics_sample)\n",
    "\n",
    "# output 1 item per line and output the first 20 samples \n",
    "for s in random_lyrics_samples[0:100]: \n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that we have generated samples of whether the song has lyrics or not. Above I have also printed the associated genre label. In many probabilistic modeling problems some information is not available to the observer. For example we could be provided only the yes/no outcomes and the genres could be \"hidden\".\n",
    "\n",
    "Now let's use these generated samples to estimate probabilities of the model. Basically we pretend that we don't know the parameters and estimate them directly by frequency counting through the samples we generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('no', 'jazz')\n",
      "('no', 'jazz')\n",
      "('no', 'jazz')\n",
      "('no', 'jazz')\n",
      "('no', 'jazz')\n",
      "('no', 'jazz')\n",
      "('no', 'jazz')\n",
      "('no', 'jazz')\n",
      "('no', 'jazz')\n",
      "('no', 'jazz')\n",
      "('no', 'jazz')\n",
      "('no', 'jazz')\n",
      "('no', 'jazz')\n",
      "('no', 'jazz')\n",
      "('yes', 'jazz')\n",
      "('no', 'jazz')\n",
      "('no', 'jazz')\n",
      "('no', 'jazz')\n",
      "('no', 'jazz')\n",
      "('yes', 'jazz')\n"
     ]
    }
   ],
   "source": [
    "# First only consider jazz samples \n",
    "jazz_samples = [x for x in random_lyrics_samples if x[1] == 'jazz']\n",
    "for s in jazz_samples[0:20]: \n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have selected the samples that are jazz we can simply count the lyrics yes and lyrics no entries and divide them by the total number of jazz samples to get estimates of the conditional probabilities. Think about the relationships: we can use the data to estimate the parameters of a model (learning), we can use the model to generate samples (generation), and we can use the model to calculate probabilities for various events (inference)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8885135135135135 0.11148648648648649\n"
     ]
    }
   ],
   "source": [
    "est_no_if_jazz = len([x for x in jazz_samples if x[0] == 'no']) / len(jazz_samples)\n",
    "est_yes_if_jazz = len([x for x in jazz_samples if x[0] == 'yes']) / len(jazz_samples)\n",
    "print(est_no_if_jazz, est_yes_if_jazz)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have seen in the slides that the probability of a song being jazz if we know that it is instrumental is 0.66.$$\n",
    "P(genre = jazz | hasLyrics = no) = \\frac{0.3 * 0.9}{0.3 * 0.9 + 0.7 * 0.2} = 0.66\n",
    "$$\n",
    "\n",
    "This is based on our knowledge of probabilities. If we have some data we can also estimate this probability directly. This is called approximate inference in contrast to the exact inference of $0.66$. When problems become complicated exact inference can become too costly to compute while approximate inference can provide reasonable answers much faster. We will see that later when examining probabilistic graphical models. As you can see in this case both the exact and approximate inference probability estimates are relatively close."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6778350515463918\n"
     ]
    }
   ],
   "source": [
    "no_samples = [x for x in random_lyrics_samples if x[0] == 'no']\n",
    "est_jazz_if_no_lyrics = len([x for x in no_samples if x[1] == 'jazz']) / len(no_samples)\n",
    "print(est_jazz_if_no_lyrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayes Classification \n",
    "\n",
    "We look at the problem of classifying songs to three genres (rap, rock and country) based on a simple binary bag of words representation. First we load the data and then we take a look at it. Using our implementation of discrete random variables we generate new random songs. Finally we show how classification can be performed using Bayes Rule. The data comes for the lyrics of songs from the Million Song DataSet and was created for an assignment in my course on MIR."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data layout and visualization\n",
    "\n",
    "The data layout and the way the classifier is implemented is not general and not optimal but done for pedagogical purposes. Each genre consists of 1000 tracks and the matrix containing the data is ordered by genre. That way the instances corresponding to a genre can easily be found by the index without having to check the class label as would be the case with a general classifier.\n",
    "\n",
    "We have created a dictionary of 30 words by taking the 10 \"best\" words based on tf-idf score for each genre. Each track is represented by a binary vector (a row in the matrix) with ones for dictionary words that are in the track and 0 for words that are not. So the matrix is 3000 instances (3 * 1000 per genre) by 30 for each word in the dictionary. When visualized one can observe the block structure that shows that the the rap tracks have a lot of words from the first 10 words in the dictionary that are characteristic of rap.\n",
    "\n",
    "**NOTE** the data is conveniently arranged for this visualization. In an actual problem the rows of this matrix would be shuffled and the block structure would not be visible. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline \n",
    "import matplotlib.pyplot as plt\n",
    "import pickle \n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# load some lyrics bag of words data, binarize, separate matrix rows by genre \n",
    "data = np.load('data/data.npz', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 30) (1000, 30) (1000, 30)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x123c620b8>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "a = data['arr_0']\n",
    "a[a > 0] = 1\n",
    "labels = np.load('data/labels.npz', allow_pickle=True)\n",
    "labels = labels['arr_0']\n",
    "dictionary = pickle.load(open('data/dictionary.pck','rb'), encoding='latin1')\n",
    "word_indices = [  41, 1465,  169,  217, 1036,  188,  260,  454,  173,  728,  163,\n",
    "        151,  107,  142,   90,  141,  161,  131,   86,   73,  165,  133,\n",
    "         84,  244,  153,  126,  137,  119,   80,  224]\n",
    "words = [dictionary[r] for r in word_indices]\n",
    "\n",
    "# binary row vectors separate by genre (rap, rock, country)\n",
    "ra_rows = a[0:1000,:]\n",
    "ro_rows = a[1000:2000,:]\n",
    "co_rows = a[2000:3000,:] \n",
    "print(ra_rows.shape, ro_rows.shape, co_rows.shape)\n",
    "plt.imshow(a, aspect='auto', cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating the 30-dimensional word probability vector for each genre\n",
    "\n",
    "\n",
    "Let's calculate the word probability vector for each genre and then look at the most probable words for each genre in our data as well as how particular songs are represented as bag of words. We can calculate the probabilities of each word in the dictionary for the songs in each genre by summing the columns of the part of the matrix that corrsponds to each genre. As some words might not appear at all I have added 1.0 to both the numerator and denominator. This is a simple form of what's called additive smoothing which is a common technique to avoid zeros for any class conditional probabilities that would lead to the whole likelihood being zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.08791208791208792, 'de')\n",
      "(0.18581418581418582, 'niggaz')\n",
      "(0.43956043956043955, 'ya')\n",
      "(0.06293706293706294, 'und')\n",
      "(0.2827172827172827, 'yall')\n",
      "(0.057942057942057944, 'ich')\n",
      "(0.4125874125874126, 'fuck')\n",
      "(0.5084915084915085, 'shit')\n",
      "(0.4115884115884116, 'yo')\n",
      "(0.3126873126873127, 'bitch')\n",
      "(0.1798201798201798, 'end')\n",
      "(0.11688311688311688, 'wait')\n",
      "(0.17182817182817184, 'again')\n",
      "(0.1968031968031968, 'light')\n",
      "(0.23276723276723277, 'eye')\n",
      "(0.12087912087912088, 'noth')\n",
      "(0.11188811188811189, 'lie')\n",
      "(0.14185814185814186, 'fall')\n",
      "(0.21478521478521478, 'our')\n",
      "(0.16283716283716285, 'away')\n",
      "(0.17382617382617382, 'gone')\n",
      "(0.26973026973026976, 'good')\n",
      "(0.22477522477522477, 'night')\n",
      "(0.0959040959040959, 'blue')\n",
      "(0.18981018981018982, 'home')\n",
      "(0.1838161838161838, 'long')\n",
      "(0.24175824175824176, 'littl')\n",
      "(0.2137862137862138, 'well')\n",
      "(0.16483516483516483, 'heart')\n",
      "(0.14185814185814186, 'old')\n",
      "------\n",
      "(0.03796203796203796, 'de')\n",
      "(0.006993006993006993, 'niggaz')\n",
      "(0.04595404595404595, 'ya')\n",
      "(0.03196803196803197, 'und')\n",
      "(0.006993006993006993, 'yall')\n",
      "(0.026973026973026972, 'ich')\n",
      "(0.08791208791208792, 'fuck')\n",
      "(0.04095904095904096, 'shit')\n",
      "(0.022977022977022976, 'yo')\n",
      "(0.01898101898101898, 'bitch')\n",
      "(0.1998001998001998, 'end')\n",
      "(0.18981018981018982, 'wait')\n",
      "(0.22077922077922077, 'again')\n",
      "(0.1998001998001998, 'light')\n",
      "(0.3086913086913087, 'eye')\n",
      "(0.1918081918081918, 'noth')\n",
      "(0.18581418581418582, 'lie')\n",
      "(0.22377622377622378, 'fall')\n",
      "(0.23776223776223776, 'our')\n",
      "(0.3206793206793207, 'away')\n",
      "(0.15384615384615385, 'gone')\n",
      "(0.15784215784215785, 'good')\n",
      "(0.2647352647352647, 'night')\n",
      "(0.06393606393606394, 'blue')\n",
      "(0.16083916083916083, 'home')\n",
      "(0.17882117882117882, 'long')\n",
      "(0.14785214785214784, 'littl')\n",
      "(0.1968031968031968, 'well')\n",
      "(0.26073926073926074, 'heart')\n",
      "(0.1108891108891109, 'old')\n"
     ]
    }
   ],
   "source": [
    "# calculate word counts for each genre \n",
    "word_probs_ra = (ra_rows.sum(axis=0).astype(float) + 1.0) / (len(ra_rows)+1.0)\n",
    "word_probs_ro = (ro_rows.sum(axis=0).astype(float) + 1.0) / (len(ro_rows)+1.0)\n",
    "word_probs_co = (co_rows.sum(axis=0).astype(float) + 1.0) / (len(co_rows)+1.0)\n",
    "\n",
    "# Let's llok at the word probabitites for rap music \n",
    "for w in zip(word_probs_ra, words): \n",
    "    print(w)\n",
    "print('------')\n",
    "for w in zip(word_probs_ro, words): \n",
    "    print(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking out the words in some songs using the binary representation\n",
    "\n",
    "Each row of the feature matrix contains ones for each word that is present in the song. We can view the words of \n",
    "any particular song by mapping these ones using the dictionary of words. Let's view the words in the 20th track (row of the matrix) of each genre and then look at track 250."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n",
      "RAP for trackid: ['ya', 'yo', 'end', 'wait', 'our', 'old']\n",
      "ROCK for trackid: ['again', 'light', 'fall', 'our', 'away', 'good', 'night', 'blue', 'long', 'well']\n",
      "COUNTRY for trackid: ['long', 'well', 'old']\n",
      "250\n",
      "RAP for trackid: ['niggaz', 'ya', 'yo', 'bitch', 'away', 'home', 'long', 'heart']\n",
      "ROCK for trackid: ['fuck', 'noth', 'lie', 'gone']\n",
      "COUNTRY for trackid: ['yall', 'wait', 'our', 'good', 'long', 'littl', 'well']\n"
     ]
    }
   ],
   "source": [
    "#let's look at the bag of words for three particular songs \n",
    "track_id = 20\n",
    "print(track_id)\n",
    "print(\"RAP for trackid:\",[words[i] for i,r in enumerate(ra_rows[track_id]) if r==1])\n",
    "print(\"ROCK for trackid:\",[words[i] for i,r in enumerate(ro_rows[track_id]) if r==1])\n",
    "print(\"COUNTRY for trackid:\",[words[i] for i,r in enumerate(co_rows[track_id]) if r==1])\n",
    "\n",
    "track_id = 250 \n",
    "print(track_id)\n",
    "print(\"RAP for trackid:\",[words[i] for i,r in enumerate(ra_rows[track_id]) if r==1])\n",
    "print(\"ROCK for trackid:\",[words[i] for i,r in enumerate(ro_rows[track_id]) if r==1])\n",
    "print(\"COUNTRY for trackid:\",[words[i] for i,r in enumerate(co_rows[track_id]) if r==1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['bitch', 'ya', 'shit', 'yo', 'fuck'],\n",
       " ['our', 'heart', 'night', 'away', 'eye'],\n",
       " ['littl', 'long', 'well', 'heart', 'night']]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's look at the k most probable words for each genre based on the data we have \n",
    "k = 5\n",
    "[[words[x] for x in np.argpartition(word_probs_ra, -k)[-k:]],\n",
    " [words[x] for x in np.argpartition(word_probs_ro, -k)[-k:]],\n",
    " [words[x] for x in np.argpartition(word_probs_co, -k)[-k:]]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating random songs based on our simplified representation\n",
    "\n",
    "Now let's generate some random songs represented as bag of words using the calculated word probabilities for each genre. This way we can understand better the assumptions and simplifications of this model. I simply generate 30 random number and then depending on the class-conditional probabilities for a particular genre if the number is great than the random number the corresponding word is selected for generation. This gives us a clear idea of what assumptions this Binomial Naive Bayes classifier makes. Running the cell multiple times show the variation we get from this very simple model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random rap ['de', 'niggaz', 'ya', 'yall', 'fuck', 'shit', 'yo', 'noth', 'away', 'night', 'littl', 'heart']\n",
      "Random rock ['fuck', 'eye', 'lie']\n",
      "Random country ['end', 'littl', 'well', 'old']\n"
     ]
    }
   ],
   "source": [
    "print('Random rap', [w for (i,w) in enumerate(words) if np.greater(word_probs_ra, np.random.rand(30))[i]])\n",
    "print('Random rock', [w for (i,w) in enumerate(words) if np.greater(word_probs_ro, np.random.rand(30))[i]])\n",
    "print('Random country', [w for (i,w) in enumerate(words) if np.greater(word_probs_co, np.random.rand(30))[i]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the calculated word probabilities to make a classifier\n",
    "\n",
    "Now let's look at classifying songs using a naive Bayes Bernoulli classifier. When the representation is binary vectors indicating absense or presence of words it is called a Bernoulli Naive Bayes. If the times a word appears in a document affect the classification it is called a Multinomial text classifier.\n",
    "\n",
    "To make a classification decision we simply calculate the likelihood for each genre independently by taking the products of the genre dependent word probabilities. The genere with the highest likelihood is selected as the predicted class. In a more realistic implementation log-likelihoods would be used to avoid problems with small numbers. Notice that when a word is absent the probability it is absent (1 - the probability it is present) is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calcuate likelihood separately for each word \n",
    "# using naive bayes assumption and multiply \n",
    "# typically a sum of log-likelihoods is used \n",
    "# rather than a multiplication. \n",
    "def likelihood(test_song, word_probs_for_genre): \n",
    "    probability_product = 1.0 \n",
    "    for (i,w) in enumerate(test_song): \n",
    "        if (w==1): \n",
    "            probability = word_probs_for_genre[i]\n",
    "        else: \n",
    "            probability = 1.0 - word_probs_for_genre[i]\n",
    "        probability_product *= probability \n",
    "    return probability_product"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the trained classifier to predict\n",
    "\n",
    "Now that we have a function to compute the likelihood given the parameters of a particular model in this case the model parameters are the probabilities for each word. We have three models to compare one for each genre. Given a test song we compute the three likelihoods and select the largest. We can randomly select a track from the country rows and then apply our predict function to see what it does. If you run the cell multiple times you will see that for most country tracks the prediction is correct but mistakes are made occassionally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random track id 38\n",
      "rock\n"
     ]
    }
   ],
   "source": [
    "def predict(test_song): \n",
    "    scores = [likelihood(test_song, word_probs_ra), \n",
    "             likelihood(test_song, word_probs_ro),\n",
    "             likelihood(test_song, word_probs_co)]\n",
    "    labels = ['rap', 'rock', 'country']\n",
    "    return labels[np.argmax(scores)]\n",
    "\n",
    "\n",
    "# predict a random country track \n",
    "track_id = np.random.randint(1000)\n",
    "print(\"Random track id\", track_id)\n",
    "test_song = co_rows[track_id]\n",
    "print(predict(test_song))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performing a simple evaluation of our classifier\n",
    "\n",
    "\n",
    "We can now write a function that given a test set and associated ground truth lables runs our Bernoulli classifier and calculates the associated classification accuracy. We can now check how well the classifier does for each subset of the data corresponding to the three genres. Using the data used to trained the classifier for testing as we do here is a methodological mistake and in a more realistic scenario or application a separate dataset would be used for testing and the processing could be repeated multiple times using a scheme like k-fold cross-validation. As the purpose of this notebook is to illustrate how probabilities are used to create a Naive Bayes classifier I don't bother with that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rap accuracy% =  74.9\n",
      "Rock accuracy% =  63.1\n",
      "Country accuracy% =  70.9\n"
     ]
    }
   ],
   "source": [
    "def predict_set(test_set, ground_truth_label): \n",
    "    score = 0 \n",
    "    for r in test_set: \n",
    "        if predict(r) == ground_truth_label: \n",
    "            score += 1\n",
    "    # convert to percentage \n",
    "    return score / 10.0 \n",
    "\n",
    "\n",
    "\n",
    "# Let's evaluate how well our classifier does on the training set \n",
    "# A more proper evaluation would utilize cross-validation \n",
    "\n",
    "print(\"Rap accuracy% = \", predict_set(ra_rows, 'rap'))\n",
    "print(\"Rock accuracy% = \", predict_set(ro_rows, 'rock'))\n",
    "print(\"Country accuracy% = \", predict_set(co_rows, 'country'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes in general\n",
    "\n",
    "This notebooks explores how a simple probabilistic model based on a binary representation for a bag of words can be used for classification. This is a toy example with a lot of assumptions and conventions that make things easier in terms of implementation. In an actual implementation the number of words in the dictionary would not be given but calculated from the data, the instances would be shuffled so to calculate the probabilities the class output field of every instance would have to be examined. The number of classes would not be fixed and loops for iterating over classes and over attributes would be written. In the computation of the likelihood a log-likelihood would be used instead and the list goes on. You can find more information about this in most textbook that describe Naive Bayes classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
